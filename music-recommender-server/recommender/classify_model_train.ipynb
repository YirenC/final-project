{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1692,
     "status": "ok",
     "timestamp": 1682347952257,
     "user": {
      "displayName": "Tian Luan",
      "userId": "10714412075779710354"
     },
     "user_tz": 300
    },
    "id": "ToHCw1YfDi06",
    "outputId": "b5c7a552-94cf-4103-c648-ae10bd115ab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "#  ONLY RUN THIS CELL ON GOOGLE COLAB  #\n",
    "########################################\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "os.chdir(\"/content/gdrive/My Drive/5100project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "izrbnWE9DfLf"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347038,
     "status": "ok",
     "timestamp": 1682348631931,
     "user": {
      "displayName": "Tian Luan",
      "userId": "10714412075779710354"
     },
     "user_tz": 300
    },
    "id": "0TAvAQJADfLg",
    "outputId": "6a943e0d-df30-414c-c8c2-bf7ebc6309a9"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, data_dir, augment=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.all_mel_spectrograms = []\n",
    "        self.all_labels = []\n",
    "        self.label_to_id = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock': 9}\n",
    "        self.augment = augment\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        for root, dirs, files in os.walk(self.data_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Load audio file\n",
    "                signal, sr = librosa.load(file_path, sr=22050)\n",
    "\n",
    "                if self.augment:\n",
    "                    # Random Crop\n",
    "                    crop_length = random.randint(0, int(0.1*sr))\n",
    "                    signal = signal[crop_length:]\n",
    "\n",
    "                    # Add random noise\n",
    "                    noise = 0.05*np.random.randn(len(signal))\n",
    "                    signal += noise\n",
    "\n",
    "                # Convert to Mel Spectrogram\n",
    "                mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "\n",
    "                # Fix length\n",
    "                mel_spectrogram = librosa.util.fix_length(mel_spectrogram, size=1292)\n",
    "\n",
    "                # Add feature\n",
    "                self.all_mel_spectrograms.append(mel_spectrogram)\n",
    "\n",
    "                # Add label\n",
    "                label = os.path.basename(root)\n",
    "                self.all_labels.append(self.label_to_id[label])\n",
    "        # verify if all the mel spectrograms have the same shape\n",
    "        # print(set([ms.shape for ms in self.all_mel_spectrograms]))\n",
    "        assert len(set([ms.shape for ms in self.all_mel_spectrograms])) == 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mel_spectrogram = self.all_mel_spectrograms[index]\n",
    "        label = self.all_labels[index]\n",
    "        return mel_spectrogram, label\n",
    "\n",
    "\n",
    "# The path to dataset\n",
    "data_dir = './genres'\n",
    "\n",
    "# create dataset with data augmentation\n",
    "augmented_dataset1 = MelSpectrogramDataset(data_dir, augment=True)\n",
    "augmented_dataset2 = MelSpectrogramDataset(data_dir, augment=True)\n",
    "\n",
    "\n",
    "# concatenate original dataset and augmented dataset\n",
    "original_dataset = MelSpectrogramDataset(data_dir)\n",
    "combined_dataset = ConcatDataset([original_dataset, augmented_dataset1, augmented_dataset2])\n",
    "\n",
    "# Split combined dataset into training set and test set\n",
    "train_dataset, test_dataset = train_test_split(combined_dataset, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create dataloader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AYWHbbePDfLh"
   },
   "outputs": [],
   "source": [
    "class MelSpectrogramClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MelSpectrogramClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(661504, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print(x.shape)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        # print(x.shape)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # print(x.shape)\n",
    "        x = self.dense1(x)\n",
    "        # print(x.shape)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # print(x.shape)\n",
    "        x = self.dense2(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = MelSpectrogramClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZsdCg4PvDfLh"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "executionInfo": {
     "elapsed": 15110,
     "status": "error",
     "timestamp": 1682357365192,
     "user": {
      "displayName": "Tian Luan",
      "userId": "10714412075779710354"
     },
     "user_tz": 300
    },
    "id": "xGJpMEluDfLh",
    "outputId": "f02028a7-3480-4072-c6f3-d981c8a275b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0h/_w95pmx104l87vhqfcxylljr0000gn/T/ipykernel_2943/3755016637.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data, target = torch.tensor(data), torch.tensor(target)\n",
      "/var/folders/0h/_w95pmx104l87vhqfcxylljr0000gn/T/ipykernel_2943/3755016637.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data, target = torch.tensor(data), torch.tensor(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Train Loss: 1.2964, Train Accuracy: 0.1639, Test Loss: 0.0706, Test Accuracy: 0.2083\n",
      "Epoch [2/15], Train Loss: 0.0617, Train Accuracy: 0.3667, Test Loss: 0.0545, Test Accuracy: 0.4967\n",
      "Epoch [3/15], Train Loss: 0.0397, Train Accuracy: 0.6706, Test Loss: 0.0411, Test Accuracy: 0.7033\n",
      "Epoch [4/15], Train Loss: 0.0225, Train Accuracy: 0.8572, Test Loss: 0.0337, Test Accuracy: 0.7475\n",
      "Epoch [5/15], Train Loss: 0.0151, Train Accuracy: 0.9117, Test Loss: 0.0338, Test Accuracy: 0.7725\n",
      "Epoch [6/15], Train Loss: 0.0107, Train Accuracy: 0.9400, Test Loss: 0.0316, Test Accuracy: 0.7975\n",
      "Epoch [7/15], Train Loss: 0.0080, Train Accuracy: 0.9494, Test Loss: 0.0342, Test Accuracy: 0.8000\n",
      "Epoch [8/15], Train Loss: 0.0083, Train Accuracy: 0.9439, Test Loss: 0.0377, Test Accuracy: 0.7725\n",
      "Epoch [9/15], Train Loss: 0.0070, Train Accuracy: 0.9561, Test Loss: 0.0350, Test Accuracy: 0.8008\n",
      "Epoch [10/15], Train Loss: 0.0048, Train Accuracy: 0.9694, Test Loss: 0.0364, Test Accuracy: 0.8025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, target)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# backward and optimize\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Calculate test accuracy\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Convert to tensor\n",
    "        data, target = torch.tensor(data), torch.tensor(target)\n",
    "\n",
    "        # Pass data to model\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.unsqueeze(1).float())\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        predicted = output.argmax(1)\n",
    "        train_accuracy += (predicted == target).sum().item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy /= len(train_loader.dataset)\n",
    "\n",
    "    # Evaluate model\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Convert data and label to tensor\n",
    "            data, target = torch.tensor(data), torch.tensor(target)\n",
    "\n",
    "            # Pass data to model\n",
    "            output = model(data.unsqueeze(1).float())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = output.argmax(1)\n",
    "            test_accuracy += (predicted == target).sum().item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy /= len(test_loader.dataset)\n",
    "\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.4f}, Test Loss: {:.4f}, Test Accuracy: {:.4f}'.format(epoch+1, num_epochs, train_loss, train_accuracy, test_loss, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Wi9hBhC-Nj3l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "# save model to target path\n",
    "torch.save(model.state_dict(), './backend/recommender/classify_model.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
